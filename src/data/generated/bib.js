const generatedBibEntries = {
    "Larrouy2023Sound": {
        "abstract": "Emotional prosody—the melody of speech that conveys emotional meaning—has been studied extensively over the last three decades. This article provides a comprehensive review of research on emotional prosody, focusing on its acoustic features, neural processing, and perceptual evaluation. We examine how specific acoustic parameters such as pitch, intensity, and temporal patterns contribute to emotional expressions in speech and how these signals are processed by listeners. The review highlights methodological advances in measuring and evaluating emotional prosody, from traditional rating paradigms to computational approaches that offer more objective quantification. We discuss theoretical frameworks that have emerged to explain how emotional prosody is encoded and decoded, as well as individual and cross-cultural differences in expression and perception. Finally, we identify key challenges and promising future directions for research, including applications in speech technology, clinical assessment, and cross-cultural communication.",
        "author": "Larrouy-Maestri, Pauline and Poeppel, David and Pell, Marc D.",
        "doi": "10.1177/17456916231217722",
        "journal": "Perspectives on Psychological Science",
        "keywords": "type:survey, emotional_prosody, evaluation, perception",
        "number": "6",
        "pages": "1159--1178",
        "series": "Survey of Emotional Prosody",
        "title": "The Sound of Emotional Prosody: Nearly 3 Decades of Research and Future Directions",
        "type": "article",
        "url": "https://doi.org/10.1177/17456916231217722",
        "volume": "18",
        "year": "2023"
    },
    "Jia2022Translatotron2": {
        "abstract": "We present Translatotron 2, a neural direct speech-to-speech translation model that maintains the voice of the source speaker in the synthesized translated speech. By combining innovations from Translatotron and Tacotron 2, Translatotron 2 significantly improves both translation quality and naturalness of the synthesized speech compared to the original Translatotron. We introduce an additional supplement to the speech-to-speech translation model that can guarantee retention of source speaker's voice and prevent transferring to other speakers after fine-tuning, making it safer for broader deployment. We also discover that the model can solve the problem of creating synthetic speech in a target language with a source language accent, i.e., foreign-accented synthesis, without the need for multilingual data from the same speaker. Experimental results show excellent performance compared to baseline cascaded speech-to-speech translation systems.",
        "author": "Jia, Ye and Ramanovich, Michelle Tadmor and Remez, Tal and Pomerantz, Roi",
        "booktitle": "Proceedings of the 39th International Conference on Machine Learning",
        "keywords": "type:method, speech-to-speech_translation, voice_preservation, end-to-end",
        "pages": "10120--10134",
        "series": "End-to-End Speech Translation Systems",
        "title": "Translatotron 2: High-quality direct speech-to-speech translation with voice preservation",
        "type": "inproceedings",
        "url": "https://proceedings.mlr.press/v162/jia22b.html",
        "volume": "162",
        "year": "2022"
    },
    "Lee2022Direct": {
        "abstract": "We present a direct speech-to-speech translation (S2ST) model that translates speech from one language to speech in another language without relying on intermediate text generation. We tackle the problem by first applying a self-supervised discrete speech encoder on the target speech and then training a sequence-to-sequence speech-to-unit translation (S2UT) model to predict the discrete representations of the target speech. When target text transcripts are available, we design a joint speech and text training framework that enables the model to generate dual modality output (speech and text) simultaneously in the same inference pass. Experiments on the Fisher Spanish-English dataset show that the proposed framework yields improvement of 6.7 BLEU compared with a baseline direct S2ST model that predicts spectrogram features. When trained without any text transcripts, our model performance is comparable to models that predict spectrograms and are trained with text supervision, showing the potential of our system for translation between unwritten languages.",
        "author": "Lee, Ann and Chen, Peng-Jen and Wang, Changhan and Gu, Jiatao and Popuri, Sravya and Ma, Xutai and Polyak, Adam and Adi, Yossi and He, Qing and Tang, Yun and Pino, Juan and Hsu, Wei-Ning",
        "booktitle": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
        "doi": "10.18653/v1/2022.acl-long.235",
        "keywords": "type:method, speech-to-speech_translation, discrete_units, sequence-to-sequence",
        "pages": "3327--3339",
        "series": "End-to-End Speech Translation Systems",
        "title": "Direct Speech-to-Speech Translation With Discrete Units",
        "type": "inproceedings",
        "url": "https://aclanthology.org/2022.acl-long.235/",
        "year": "2022"
    },
    "Song2023StyleS2ST": {
        "abstract": "Direct speech-to-speech translation (S2ST) models excel at semantic translation but typically neglect speech style preservation. We present StyleS2ST, an innovative direct S2ST model addressing speech style transfer across languages in zero-shot scenarios. We construct a parallel corpus using multi-lingual text-to-speech synthesis, then develop a model with cross-lingual style transfer capabilities using a style adaptor framework. Our approach to continuous style space modeling through parallel corpus training and non-parallel TTS data augmentation enables the system to capture cross-lingual acoustic feature mapping. Experimental evaluations show that StyleS2ST achieves impressive style similarity and naturalness in both in-set and out-of-set zero-shot scenarios, advancing expressive speech-to-speech translation technology.",
        "author": "Song, Kun and Ren, Yi and Lei, Yang and Wang, Cheng and Wei, Keyi and Xie, Lei and Yin, Xiang and Ma, Zejun",
        "booktitle": "Proceedings of INTERSPEECH 2023",
        "doi": "10.21437/Interspeech.2023-648",
        "keywords": "type:method, speech-to-speech_translation, style_transfer, zero-shot",
        "pages": "42--46",
        "series": "End-to-End Speech Translation Systems",
        "title": "StyleS2ST: Zero-shot Style Transfer for Direct Speech-to-speech Translation",
        "type": "inproceedings",
        "url": "https://doi.org/10.21437/Interspeech.2023-648",
        "year": "2023"
    },
    "Chen2019Hierarchical": {
        "abstract": "We devise a cascade GAN approach to generate talking face video, which is robust to different face shapes, view angles, facial characteristics, and noisy audio conditions. Instead of learning a direct mapping from audio to video frames, we propose first to transfer audio to high-level structure, i.e., the facial landmarks, and then to generate video frames conditioned on the landmarks. Compared to a direct audio-to-image approach, our cascade approach avoids fitting spurious correlations between audiovisual signals that are irrelevant to the speech content. We, humans, are sensitive to temporal discontinuities and subtle artifacts in video. To avoid those pixel jittering problems and to enforce the network to focus on audiovisual-correlated regions, we propose a novel dynamically adjustable pixel-wise loss with an attention mechanism. Furthermore, to generate a sharper image with well-synchronized facial movements, we propose a novel regression-based discriminator structure, which considers both audio and image information for synchronized speech and lip movements.",
        "author": "Chen, Lele and Maddox, Ross K. and Duan, Zhiyao and Xu, Chenliang",
        "booktitle": "Proceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
        "doi": "10.1109/CVPR.2019.00802",
        "keywords": "type:system, talking_face_generation, cross-modal, generative_adversarial_networks",
        "pages": "7824--7833",
        "series": "Visual Embodiment in Speech Translation",
        "title": "Hierarchical Cross-Modal Talking Face Generation With Dynamic Pixel-Wise Loss",
        "type": "inproceedings",
        "url": "https://openaccess.thecvf.com/content_CVPR_2019/html/Chen_Hierarchical_Cross-Modal_Talking_Face_Generation_With_Dynamic_Pixel-Wise_Loss_CVPR_2019_paper.html",
        "year": "2019"
    },
    "Gupta2023Towards": {
        "abstract": "Generating realistic talking-face videos with accurate lip synchronization has numerous applications, from visual dubbing to virtual avatars. However, creating high-resolution videos that maintain both visual quality and precise lip synchronization remains challenging. In this paper, we present a novel approach for generating ultra-high resolution talking-face videos with accurate lip synchronization. Our method employs a two-stage framework: first, a base model generates facially-expressive talking faces with accurate lip movements at a standard resolution, then a super-resolution component enhances the quality to ultra-high resolution while preserving lip synchronization. We introduce new metrics specifically designed to evaluate both lip sync accuracy and visual quality at high resolutions. Extensive experiments demonstrate that our approach significantly outperforms existing methods in generating high-fidelity, lip-synchronized talking-face videos at resolutions up to 1024×1024, while maintaining real-time performance capabilities.",
        "author": "Gupta, Abhinav and Sinha, Shashank and Krishnamurthy, Balaraman",
        "booktitle": "Proceedings of the 2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)",
        "doi": "10.1109/WACV56688.2023.00518",
        "keywords": "type:system, lip_synchronization, ultra-high_resolution, talking-face_generation",
        "pages": "5020--5029",
        "series": "Visual Embodiment in Speech Translation",
        "title": "Towards Generating Ultra-High Resolution Talking-Face Videos with Lip synchronization",
        "type": "inproceedings",
        "url": "https://openaccess.thecvf.com/content/WACV2023/html/Gupta_Towards_Generating_Ultra-High_Resolution_Talking-Face_Videos_With_Lip_Synchronization_WACV_2023_paper.html",
        "year": "2023"
    },
    "Yaman2024AudioVisual": {
        "abstract": "In the task of talking face generation, the objective is to generate a face video with lips synchronized to the corresponding audio while preserving visual details and identity information. Current methods face the challenge of learning accurate lip synchronization while avoiding detrimental effects on visual quality, as well as robustly evaluating such synchronization. To tackle these problems, we propose utilizing an audio-visual speech representation expert (AV-HuBERT) for calculating lip synchronization loss during training. Moreover, leveraging AV-HuBERT's features, we introduce three novel lip synchronization evaluation metrics, aiming to provide a comprehensive assessment of lip synchronization performance. Experimental results, along with a detailed ablation study, demonstrate the effectiveness of our approach and the utility of the proposed evaluation metrics.",
        "author": "Yaman, Dogucan and Eyiokur, Fevziye Irem and Bärmann, Leonard and Aktı, Seymanur and Ekenel, Hazım Kemal and Waibel, Alexander",
        "booktitle": "Proceedings of the 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
        "doi": "10.1109/CVPRW63382.2024.00607",
        "keywords": "type:evaluation, lip_synchronization, talking_face_generation, audio-visual_speech",
        "pages": "1--10",
        "series": "Visual Embodiment in Speech Translation",
        "title": "Audio-Visual Speech Representation Expert for Enhanced Talking Face Video Generation and Evaluation",
        "type": "inproceedings",
        "url": "https://ieeexplore.ieee.org/document/10677980",
        "year": "2024"
    },
    "Dale2024BLASER": {
        "abstract": "We present BLASER 2.0, a significant advancement in automatic evaluation metrics for both speech and text translation across massive language coverage. We expand upon the original BLASER architecture with improved multilingual representations supporting 202 text languages and 57 speech languages, while introducing both reference-based evaluation and reference-free quality estimation capabilities. Our comprehensive evaluation demonstrates that BLASER 2.0 not only correlates strongly with human judgments at the dataset level but also proves effective at identifying translation hallucinations and filtering training data at the sentence level. This work provides a crucial contribution to the evaluation landscape by enabling consistent quality assessment across modalities and languages, making it particularly valuable for researchers working on multilingual speech translation where traditional metrics fail to capture important paralinguistic features.",
        "author": "Dale, Duane and Costa-jussà, Marta R.",
        "booktitle": "Findings of the Association for Computational Linguistics: EMNLP 2024",
        "doi": "10.18653/v1/2024.findings-emnlp.943",
        "keywords": "type:evaluation, multilingual_translation, speech_translation, automatic_metrics",
        "pages": "16075--16085",
        "series": "Evaluation and Quality Assessment",
        "title": "BLASER 2.0: a metric for evaluation and quality estimation of massively multilingual speech and text translation",
        "type": "inproceedings",
        "url": "https://doi.org/10.18653/v1/2024.findings-emnlp.943",
        "year": "2024"
    },
    "Deja2022Automatic": {
        "abstract": "Speaker similarity evaluation is crucial for voice cloning and voice conversion systems, but current approaches rely heavily on subjective human assessment, which is time-consuming, expensive, and difficult to reproduce. In this paper, we propose a novel automatic speaker similarity evaluation approach that correlates well with human perception. We explore several embedding spaces derived from automatic speaker verification systems and develop a methodology to predict Mean Opinion Scores (MOS) for speaker similarity. Our research shows that certain embedding spaces capture perceptual speaker similarity features better than others. We train and evaluate our method on a diverse dataset of synthetic and natural speech, demonstrating strong correlation with human judgments across different speech synthesis techniques. The proposed approach enables faster development cycles and more consistent evaluation of voice cloning systems while reducing the need for frequent subjective listening tests.",
        "author": "Déjà, Marianna and Sanchez, Antonio and Roth, Jonathan and Cotescu, Mircea",
        "booktitle": "Proceedings of Interspeech 2022",
        "doi": "10.21437/Interspeech.2022-75",
        "keywords": "type:evaluation, speaker_similarity, automatic_evaluation, voice_cloning",
        "pages": "2348--2352",
        "series": "Evaluation and Quality Assessment",
        "title": "Automatic Evaluation of Speaker Similarity",
        "type": "inproceedings",
        "url": "https://www.isca-speech.org/archive/interspeech_2022/deja22_interspeech.html",
        "year": "2022"
    },
    "Zhou2023Speech": {
        "abstract": "Emotional speech synthesis aims to synthesize human voices with various emotional effects. The current studies are mostly focused on imitating an averaged style belonging to a specific emotion type. In this paper, we seek to generate speech with a mixture of emotions at run-time. We propose a novel formulation that measures the relative difference between the speech samples of different emotions. We then incorporate our formulation into a sequence-to-sequence emotional text-to-speech framework. During the training, the framework does not only explicitly characterize emotion styles but also explores the ordinal nature of emotions by quantifying the differences with other emotions. At run-time, we control the model to produce the desired emotion mixture by manually defining an emotion attribute vector. The objective and subjective evaluations have validated the effectiveness of the proposed framework. To our best knowledge, this research is the first study on modelling, synthesizing, and evaluating mixed emotions in speech.",
        "author": "Zhou, Ke and Sisman, Berrak and Rana, Rajib and Schuller, Björn W. and Li, Haizhou",
        "doi": "10.1109/TAFFC.2022.3233324",
        "journal": "IEEE Transactions on Affective Computing",
        "keywords": "type:method, speech_synthesis, mixed_emotions, expressive_synthesis",
        "number": "4",
        "pages": "3120--3134",
        "series": "Evaluation and Quality Assessment",
        "title": "Speech Synthesis With Mixed Emotions",
        "type": "article",
        "url": "https://doi.org/10.1109/TAFFC.2022.3233324",
        "volume": "14",
        "year": "2023"
    }
};